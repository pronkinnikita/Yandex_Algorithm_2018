{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import fastText\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymorphy2\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Reshape, LSTM, BatchNormalization\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Merge, Activation, Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adadelta, RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing OpenSubtitles for w2v fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(source, total):\n",
    "    processed = source + '.processed'\n",
    "\n",
    "    with open(source, encoding='utf-8') as source_f, open(processed, 'w', encoding='utf-8') as processed_f:\n",
    "        for line in tqdm_notebook(source_f, total=total):\n",
    "            #print(line.lower().translate(table))\n",
    "            processed_f.write(line.lower().translate(table))\n",
    "\n",
    "#table = str.maketrans({key: None for key in string.punctuation})\n",
    "#process('../w2v/os_dataset/OpenSubtitles2018.en-ru.ru', 25910105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = fastText.load_model('../w2v/os_model/os_model_2018.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = gensim.models.KeyedVectors.load('os_model_2018.gen.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    return np.sum(v1 * v2) / (np.sqrt(np.sum(v1 ** 2)) * np.sqrt(np.sum(v2 ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsing_Xtrain(path):\n",
    "    data_list = []\n",
    "    with open(path, encoding='utf-8') as tsv:\n",
    "        data_list = [line.split('\\t') for line in tsv]\n",
    "    data = pd.DataFrame(data_list, columns=['context_id','context_2',\n",
    "                                            'context_1','context_0',\n",
    "                                            'reply_id','reply','label',\n",
    "                                            'confidence'],)\n",
    "    return data\n",
    "\n",
    "def parsing_Xtest(path):\n",
    "    data_list = []\n",
    "    with open(path, encoding='utf-8') as tsv:\n",
    "        data_list = [line.split('\\t') for line in tsv]\n",
    "    data = pd.DataFrame(data_list, columns=['context_id','context_2',\n",
    "                                            'context_1','context_0',\n",
    "                                            'reply_id','reply'],\n",
    "                       )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def clear_text(text):        \n",
    "    text = re.sub(r'[^\\w-]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+$', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('ั', 'ะต')\n",
    "    return text\n",
    "\n",
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=2**32)\n",
    "def normalize_word(word):\n",
    "    return morph.normal_forms(word)[0]\n",
    "\n",
    "def normalize_text(text):\n",
    "    return ' '.join([normalize_word(word) for word in text.split(' ')])\n",
    "\n",
    "def clear_data(X, norm=False):\n",
    "    X.context_2 = X.context_2.map(clear_text)\n",
    "    X.context_1 = X.context_1.map(clear_text)\n",
    "    X.context_0 = X.context_0.map(clear_text)\n",
    "    X.reply = X.reply.map(clear_text)\n",
    "    if norm == True:\n",
    "        X.context_2 = X.context_2.map(normalize_text)\n",
    "        X.context_1 = X.context_1.map(normalize_text)\n",
    "        X.context_0 = X.context_0.map(normalize_text)\n",
    "        X.reply = X.reply.map(normalize_text)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = parsing_Xtrain('yac_data/train.tsv')\n",
    "Xtest = parsing_Xtest('yac_data/public.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain.label = Xtrain.label.map({'good' : int(2),\n",
    "                                 'bad' : int(0),\n",
    "                                 'neutral' : int(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.48 s, sys: 20.8 ms, total: 3.5 s\n",
      "Wall time: 3.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xtest = clear_data(Xtest, False)\n",
    "Xtrain = clear_data(Xtrain, False)\n",
    "\n",
    "y = Xtrain.label.values\n",
    "y = y.astype(np.int)\n",
    "label = to_categorical(y)\n",
    "label.shape\n",
    "\n",
    "def clear_conf(conf):\n",
    "    return conf[:-1]\n",
    "Xtrain.confidence = Xtrain.confidence.map(clear_conf)\n",
    "Xtrain.confidence = Xtrain.confidence.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for sample in Xtrain.values:\n",
    "    #print(sample)\n",
    "    corpus.append(sample[2] + ' ' + sample[1] + ' ' + sample[3] + ' ' + sample[5])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "#vectorizer.idf_\n",
    "#train = vectorizer.transform(corpus)\n",
    "#a  = pd.DataFrame(corpus).to_csv('corus.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf_sentence(sent, vocab, idf):\n",
    "    if len(sent) == 0:\n",
    "        return 0\n",
    "    sum_idf = 0\n",
    "    for word in sent:\n",
    "        if word in vocab:\n",
    "            sum_idf += idf[vocab[word]]\n",
    "        else:\n",
    "            continue\n",
    "    return sum_idf / len(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_important_word(sent, vectorizer, n=5):\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    idf = vectorizer.idf_\n",
    "    v_sent = w2v.get_sentence_vector(' '.join(sent))\n",
    "    imp_list = []\n",
    "    for word in sent:\n",
    "        #v_word = w2v.get_word_vector(word)\n",
    "        #score = similarity(v_sent, v_word)\n",
    "        if word in vocab:\n",
    "            score = idf[vocab[word]]\n",
    "            imp_list.append((score, word))\n",
    "        else:\n",
    "            continue\n",
    "    imp_list.sort(reverse=True)\n",
    "    if len(imp_list) >= n:\n",
    "        th = imp_list[n - 1][0]\n",
    "    else:\n",
    "        th = 0\n",
    "    imp_list = []\n",
    "    for word in sent:\n",
    "        if word in vocab:\n",
    "            score = idf[vocab[word]]\n",
    "            #print(score, th)\n",
    "            if score >= th:\n",
    "                imp_list.append(word)\n",
    "        else:\n",
    "            continue\n",
    "    #print(imp_list[:5])\n",
    "    return imp_list[:5]\n",
    "\n",
    "def get_important_words(X, vectorizer, w2v):\n",
    "    data = []\n",
    "    print('Getting data...')\n",
    "    for sample in tqdm_notebook(X.values):\n",
    "        c2 = sample[1].split()\n",
    "        c1 = sample[2].split()\n",
    "        c0 = sample[3].split()\n",
    "        r = sample[5].split()\n",
    "        \n",
    "        c2 = get_top_important_word(c2, vectorizer)\n",
    "        c1 = get_top_important_word(c1, vectorizer)\n",
    "        c0 = get_top_important_word(c0, vectorizer)\n",
    "        r = get_top_important_word(r, vectorizer)\n",
    "        \n",
    "        c2_f = np.zeros((5, 200))\n",
    "        c1_f = np.zeros((5, 200))\n",
    "        c0_f = np.zeros((5, 200))\n",
    "        r_f = np.zeros((5, 200))\n",
    "        for i in range(len(c2)):\n",
    "            c2_f[i] = w2v.get_word_vector(c2[i])\n",
    "        for i in range(len(c1)):\n",
    "            c1_f[i] = w2v.get_word_vector(c1[i])\n",
    "        for i in range(len(c0)):\n",
    "            c0_f[i] = w2v.get_word_vector(c0[i])\n",
    "        for i in range(len(r)):\n",
    "            r_f[i] = w2v.get_word_vector(r[i])\n",
    "            \n",
    "        data.append((c2_f, c1_f, c0_f, r_f))\n",
    "        \n",
    "    print('...Done')\n",
    "    data = pd.DataFrame(data, columns=['c2', 'c1', 'c0', 'r'])\n",
    "\n",
    "    c2 = np.zeros((len(data), 5, 200))\n",
    "    print('Progress: 0%')\n",
    "    for i in range(len(data)):\n",
    "        c2[i] = data.c2[i]\n",
    "    c1 = np.zeros((len(data), 5, 200))\n",
    "    print('Progress: 25%')\n",
    "    for i in range(len(data)):\n",
    "        c1[i] = data.c1[i]\n",
    "    c0 = np.zeros((len(data), 5, 200))\n",
    "    print('Progress: 50%')\n",
    "    for i in range(len(data)):\n",
    "        c0[i] = data.c0[i]\n",
    "    r = np.zeros((len(data), 5, 200))\n",
    "    print('Progress: 75%')\n",
    "    for i in range(len(data)):\n",
    "        r[i] = data.r[i]\n",
    "    print('Progress: 100%')\n",
    "    return c2,c1,c0,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aeea30baeeb496cba6dde1afe9aa985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=97533), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Done\n",
      "Progress: 0%\n",
      "Progress: 25%\n",
      "Progress: 50%\n",
      "Progress: 75%\n",
      "Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "#top_c2, top_c1 = np.load('yac_data/top_words/top_c2_full.npy'), np.load('yac_data/top_words/top_c1_full.npy')\n",
    "#top_c0, top_r = np.load('yac_data/top_words/top_c0_full.npy'), np.load('yac_data/top_words/top_r_full.npy')\n",
    "\n",
    "#t_top_c2, t_top_c1 = np.load('yac_data/top_words/t_top_c2_full.npy'), np.load('yac_data/top_words/t_top_c1_full.npy')\n",
    "#t_top_c0, t_top_r = np.load('yac_data/top_words/t_top_c0_full.npy'), np.load('yac_data/top_words/t_top_r_full.npy')\n",
    "#t_top = np.concatenate([t_top_c2, t_top_c1, t_top_c0, t_top_r], axis=1)\n",
    "top_c2, top_c1, top_c0, top_r = get_important_words(Xtrain, vectorizer, w2v)\n",
    "top = np.concatenate([top_c2, top_c1, top_c0, top_r], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting grammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grammes_keys = get_grammems_keys(Xtrain)\n",
    "#grammes_keys = np.load('grammes_keys.npy')\n",
    "#grammes_keys = list(grammes_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_grammems_sentence(sent, morph):\n",
    "    if len(sent) == 0:\n",
    "        return {}\n",
    "    grammems = {}\n",
    "    for word in sent:\n",
    "        tags = list(morph.parse(word)[0].tag.grammemes)\n",
    "        for tag in tags:\n",
    "            if tag in grammems:\n",
    "                grammems[tag] += 1 / len(sent)\n",
    "            else:\n",
    "                grammems[tag] = 1 / len(sent)\n",
    "    return grammems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gram_c2, gram_c1, gram_c0, gram_r = get_grammems(Xtrain, list(grammes_keys))\n",
    "gram_c2, gram_c1 = np.load('yac_data/grammems/gram_c2.npy'), np.load('yac_data/grammems/gram_c1.npy')\n",
    "gram_c0, gram_r = np.load('yac_data/grammems/gram_c0.npy'), np.load('yac_data/grammems/gram_r.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t_gram_c2, t_gram_c1, t_gram_c0, t_gram_r = get_grammems(Xtest, list(grammes_keys))\n",
    "t_gram_c2, t_gram_c1 = np.load('yac_data/grammems/t_gram_c2.npy'), np.load('yac_data/grammems/t_gram_c1.npy')\n",
    "t_gram_c0, t_gram_r = np.load('yac_data/grammems/t_gram_c0.npy'), np.load('yac_data/grammems/t_gram_r.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf(X, vectorizer):\n",
    "    data = []\n",
    "    print('Getting data...')\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    idf = vectorizer.idf_\n",
    "    for sample in tqdm_notebook(X.values):\n",
    "        idf_c2 = get_idf_sentence(sample[1].split(), vocab, idf)\n",
    "        cw_c2 = len(sample[1])\n",
    "        c2 = np.array([idf_c2, cw_c2])\n",
    "            \n",
    "        idf_c1 = get_idf_sentence(sample[2].split(), vocab, idf)\n",
    "        cw_c1 = len(sample[2])\n",
    "        c1 = np.array([idf_c1, cw_c1])\n",
    "            \n",
    "        idf_c0 = get_idf_sentence(sample[3].split(), vocab, idf)\n",
    "        cw_c0 = len(sample[3])\n",
    "        c0 = np.array([idf_c0, cw_c0])\n",
    "            \n",
    "        idf_r = get_idf_sentence(sample[5].split(), vocab, idf)\n",
    "        cw_r = len(sample[5])\n",
    "        r = np.array([idf_r, cw_r])\n",
    "        \n",
    "        data.append((c2, c1, c0, r))\n",
    "        \n",
    "    print('...Done')\n",
    "    data = pd.DataFrame(data, columns=['c2', 'c1', 'c0', 'r'])\n",
    "\n",
    "    c2 = np.zeros((len(data), 2))\n",
    "    print('Progress: 0%')\n",
    "    for i in range(len(data)):\n",
    "        c2[i] = data.c2[i]\n",
    "    c1 = np.zeros((len(data), 2))\n",
    "    print('Progress: 25%')\n",
    "    for i in range(len(data)):\n",
    "        c1[i] = data.c1[i]\n",
    "    c0 = np.zeros((len(data), 2))\n",
    "    print('Progress: 50%')\n",
    "    for i in range(len(data)):\n",
    "        c0[i] = data.c0[i]\n",
    "    r = np.zeros((len(data), 2))\n",
    "    print('Progress: 75%')\n",
    "    for i in range(len(data)):\n",
    "        r[i] = data.r[i]\n",
    "    print('Progress: 100%')\n",
    "    return c2,c1,c0,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8c0f4d99d64a0e885ea8b4558b12e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=97533), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Done\n",
      "Progress: 0%\n",
      "Progress: 25%\n",
      "Progress: 50%\n",
      "Progress: 75%\n",
      "Progress: 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97533, 8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_c2, meta_c1, meta_c0, meta_r = get_idf(Xtrain, vectorizer)\n",
    "meta_x = np.concatenate([meta_c2, meta_c1, meta_c0, meta_r], axis=1)\n",
    "meta_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_c2_s = meta_c2.reshape(meta_c2.shape[0], 1, 2)\n",
    "meta_c1_s = meta_c1.reshape(meta_c1.shape[0], 1, 2)\n",
    "meta_c0_s = meta_c0.reshape(meta_c0.shape[0], 1, 2)\n",
    "meta_r_s = meta_r.reshape(meta_r.shape[0], 1, 2)\n",
    "\n",
    "meta_x_s = np.concatenate([meta_c2_s, meta_c1_s, meta_c0_s, meta_r_s], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97533, 4, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_x_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef579576b6e40f5b6643f2a025d4619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9968), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Done\n",
      "Progress: 0%\n",
      "Progress: 25%\n",
      "Progress: 50%\n",
      "Progress: 75%\n",
      "Progress: 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9968, 8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_meta_c2, t_meta_c1, t_meta_c0, t_meta_r = get_idf(Xtest, vectorizer)\n",
    "t_meta_x = np.concatenate([t_meta_c2, t_meta_c1, t_meta_c0, t_meta_r], axis=1)\n",
    "t_meta_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_meta_c2_s = t_meta_c2.reshape(t_meta_c2.shape[0], 1, 2)\n",
    "t_meta_c1_s = t_meta_c1.reshape(t_meta_c1.shape[0], 1, 2)\n",
    "t_meta_c0_s = t_meta_c0.reshape(t_meta_c0.shape[0], 1, 2)\n",
    "t_meta_r_s = t_meta_r.reshape(t_meta_r.shape[0], 1, 2)\n",
    "\n",
    "t_meta_x_s = np.concatenate([t_meta_c2_s, t_meta_c1_s, t_meta_c0_s, t_meta_r_s], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting w2v sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_data(X, vectorizer=None, dim=100, do_idf=False):\n",
    "    data = []\n",
    "    print('Getting data...')\n",
    "    if do_idf == True:\n",
    "        vocab = vectorizer.vocabulary_\n",
    "        idf = vectorizer.idf_\n",
    "        dim += 2\n",
    "    for sample in tqdm_notebook(X.values):\n",
    "        c2 = w2v.get_sentence_vector(sample[1])\n",
    "        c1 = w2v.get_sentence_vector(sample[2])\n",
    "        c0 = w2v.get_sentence_vector(sample[3])\n",
    "        r = w2v.get_sentence_vector(sample[5])\n",
    "        \n",
    "        if do_idf == True:\n",
    "            idf_c2 = get_idf_sentence(sample[1].split(), vocab, idf)\n",
    "            cw_c2 = len(sample[1])\n",
    "            c2 = np.concatenate([c2, [idf_c2, cw_c2]])\n",
    "            \n",
    "            idf_c1 = get_idf_sentence(sample[2].split(), vocab, idf)\n",
    "            cw_c1 = len(sample[2])\n",
    "            c1 = np.concatenate([c1, [idf_c1, cw_c1]])\n",
    "            \n",
    "            idf_c0 = get_idf_sentence(sample[3].split(), vocab, idf)\n",
    "            cw_c0 = len(sample[3])\n",
    "            c0 = np.concatenate([c0, [idf_c0, cw_c0]])\n",
    "            \n",
    "            idf_r = get_idf_sentence(sample[5].split(), vocab, idf)\n",
    "            cw_r = len(sample[5])\n",
    "            r = np.concatenate([r, [idf_r, cw_r]])\n",
    "        \n",
    "        data.append((c2, c1, c0, r))\n",
    "        \n",
    "    print('...Done')\n",
    "    data = pd.DataFrame(data, columns=['c2', 'c1', 'c0', 'r'])\n",
    "\n",
    "    c2 = np.zeros((len(data), dim))\n",
    "    print('Progress: 0%')\n",
    "    for i in range(len(data)):\n",
    "        c2[i] = data.c2[i]\n",
    "    c1 = np.zeros((len(data), dim))\n",
    "    print('Progress: 25%')\n",
    "    for i in range(len(data)):\n",
    "        c1[i] = data.c1[i]\n",
    "    c0 = np.zeros((len(data), dim))\n",
    "    print('Progress: 50%')\n",
    "    for i in range(len(data)):\n",
    "        c0[i] = data.c0[i]\n",
    "    r = np.zeros((len(data), dim))\n",
    "    print('Progress: 75%')\n",
    "    for i in range(len(data)):\n",
    "        r[i] = data.r[i]\n",
    "    print('Progress: 100%')\n",
    "    return c2,c1,c0,r\n",
    "\n",
    "def get_intersection(X, vectorizer=None, dim=100, do_syn=False, do_idf=False):\n",
    "    data = []\n",
    "    print('Getting data...')\n",
    "    if do_idf == True:\n",
    "        vocab = vectorizer.vocabulary_\n",
    "        idf = vectorizer.idf_\n",
    "        dim += 2\n",
    "    syn_vocab = {}\n",
    "    for sample in tqdm_notebook(X.values):\n",
    "        c2 = set(sample[1].split())\n",
    "        c1 = set(sample[2].split())\n",
    "        c0 = set(sample[3].split())\n",
    "        if do_syn == False:\n",
    "            r = set(sample[5].split())\n",
    "        else:\n",
    "            r = sample[5].split()\n",
    "            r_extended = []\n",
    "            for w_r in r:\n",
    "                if w_r in syn_vocab:\n",
    "                    synonyms = syn_vocab[w_r]\n",
    "                else:\n",
    "                    v_r = w2v.get_word_vector(w_r)\n",
    "                    synonyms = m.similar_by_vector(v_r, topn=4)\n",
    "                    syn_vocab[w_r] = synonyms\n",
    "                #print(synonyms)\n",
    "                synonyms = pd.DataFrame(synonyms, columns=['word', 'prob'])\n",
    "                synonyms = synonyms.word.tolist()\n",
    "                r_extended += synonyms\n",
    "            r = set(r_extended)\n",
    "        \n",
    "        list_c2 = list(c2 & r)\n",
    "        list_c1 = list(c1 & r)\n",
    "        list_c0 = list(c0 & r)\n",
    "        inter_c2 = w2v.get_sentence_vector(' '.join(list_c2))\n",
    "        inter_c1 = w2v.get_sentence_vector(' '.join(list_c1))\n",
    "        inter_c0 = w2v.get_sentence_vector(' '.join(list_c0))\n",
    "        \n",
    "        if do_idf == True:\n",
    "            idf_c2 = get_idf_sentence(list_c2, vocab, idf)\n",
    "            cw_c2 = len(list_c2)\n",
    "            inter_c2 = np.concatenate([inter_c2, [idf_c2, cw_c2]])\n",
    "            \n",
    "            idf_c1 = get_idf_sentence(list_c1, vocab, idf)\n",
    "            cw_c1 = len(list_c1)\n",
    "            inter_c1 = np.concatenate([inter_c1, [idf_c1, cw_c1]])\n",
    "            \n",
    "            idf_c0 = get_idf_sentence(list_c0, vocab, idf)\n",
    "            cw_c0 = len(list_c0)\n",
    "            inter_c0 = np.concatenate([inter_c0, [idf_c0, cw_c0]])\n",
    "        \n",
    "        data.append((inter_c2, inter_c1, inter_c0))\n",
    "    print('...Done')\n",
    "    data = pd.DataFrame(data, columns=['c2', 'c1', 'c0'])\n",
    "\n",
    "    c2 = np.zeros((len(data), dim))\n",
    "    print('Progress: 0%')\n",
    "    for i in range(len(data)):\n",
    "        c2[i] = data.c2[i]\n",
    "    c1 = np.zeros((len(data), dim))\n",
    "    print('Progress: 30%')\n",
    "    for i in range(len(data)):\n",
    "        c1[i] = data.c1[i]\n",
    "    c0 = np.zeros((len(data), dim))\n",
    "    print('Progress: 65%')\n",
    "    for i in range(len(data)):\n",
    "        c0[i] = data.c0[i]\n",
    "    print('Progress: 100%')\n",
    "    return c2, c1, c0\n",
    "\n",
    "def get_intersection_with_context(X, dim=100, do_idf=False):\n",
    "    data = []\n",
    "    print('Getting data...')\n",
    "    if do_idf == True:\n",
    "        vocab = vectorizer.vocabulary_\n",
    "        idf = vectorizer.idf_\n",
    "        dim += 2\n",
    "    for sample in tqdm_notebook(X.values):\n",
    "        c2 = set(sample[1].split())\n",
    "        c1 = set(sample[2].split())\n",
    "        c0 = set(sample[3].split())\n",
    "        i1 = list(c2 & c1)\n",
    "        i2 = list(c1 & c0)\n",
    "        i3 = list(c0 & c2)\n",
    "        \n",
    "        inter_c2_c1 = w2v.get_sentence_vector(' '.join(i1))\n",
    "        inter_c1_c0 = w2v.get_sentence_vector(' '.join(i2))\n",
    "        inter_c0_c2 = w2v.get_sentence_vector(' '.join(i3))\n",
    "        if do_idf == True:\n",
    "            idf_c2 = get_idf_sentence(i1, vocab, idf)\n",
    "            cw_c2 = len(i1)\n",
    "            inter_c2_c1 = np.concatenate([inter_c2_c1, [idf_c2, cw_c2]])\n",
    "            \n",
    "            idf_c1 = get_idf_sentence(i2, vocab, idf)\n",
    "            cw_c1 = len(i2)\n",
    "            inter_c1_c0 = np.concatenate([inter_c1_c0, [idf_c1, cw_c1]])\n",
    "            \n",
    "            idf_c0 = get_idf_sentence(i3, vocab, idf)\n",
    "            cw_c0 = len(i3)\n",
    "            inter_c0_c2 = np.concatenate([inter_c0_c2, [idf_c0, cw_c0]])\n",
    "        \n",
    "        data.append((inter_c2_c1, inter_c1_c0, inter_c0_c2))\n",
    "    print('...Done')\n",
    "    data = pd.DataFrame(data, columns=['c2_c1', 'c1_c0', 'c0_c2'])\n",
    "\n",
    "    c2_c1 = np.zeros((len(data), dim))\n",
    "    print('Progress: 0%')\n",
    "    for i in range(len(data)):\n",
    "        c2_c1[i] = data.c2_c1[i]\n",
    "    c1_c0 = np.zeros((len(data), dim))\n",
    "    print('Progress: 30%')\n",
    "    for i in range(len(data)):\n",
    "        c1_c0[i] = data.c1_c0[i]\n",
    "    c0_c2 = np.zeros((len(data), dim))\n",
    "    print('Progress: 65%')\n",
    "    for i in range(len(data)):\n",
    "        c0_c2[i] = data.c0_c2[i]\n",
    "    print('Progress: 100%')\n",
    "    return c2_c1, c1_c0, c0_c2\n",
    "\n",
    "def get_grammems_keys(X):\n",
    "    keys = set()\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    print('Getting data...')\n",
    "    for sample in tqdm_notebook(X.values):\n",
    "        grammems_c2 = get_grammems_sentence(sample[1].split(), morph)\n",
    "        grammems_c1 = get_grammems_sentence(sample[2].split(), morph)\n",
    "        grammems_c0 = get_grammems_sentence(sample[3].split(), morph)\n",
    "        grammems_r = get_grammems_sentence(sample[5].split(), morph)\n",
    "        \n",
    "        keys.update((list(grammems_c2.keys())))\n",
    "        keys.update(list(grammems_c1.keys()))\n",
    "        keys.update((list(grammems_c0.keys())))\n",
    "        keys.update((list(grammems_r.keys())))\n",
    "            \n",
    "    print('...Done')\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb43cd0f8a646228062fa174ab0e6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=97533), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Done\n",
      "Progress: 0%\n",
      "Progress: 25%\n",
      "Progress: 50%\n",
      "Progress: 75%\n",
      "Progress: 100%\n",
      "CPU times: user 22.2 s, sys: 6.74 s, total: 28.9 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c2, c1, c0, r = get_w2v_data(Xtrain, vectorizer, 200)\n",
    "#c2, c1 = np.load('yac_data/sums/c2.npy'), np.load('yac_data/sums/c1.npy')\n",
    "#c0, r = np.load('yac_data/sums/c0.npy'), np.load('yac_data/sums/r.npy')\n",
    "\n",
    "#t_c2, t_c1 = np.load('yac_data/sums/t_c2.npy'), np.load('yac_data/sums/t_c1.npy')\n",
    "#t_c0, t_r = np.load('yac_data/sums/t_c0.npy'), np.load('yac_data/sums/t_r.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching intersections with reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d882b3d79b4de8954be3445ba3c6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=97533), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Done\n",
      "Progress: 0%\n",
      "Progress: 30%\n",
      "Progress: 65%\n",
      "Progress: 100%\n",
      "Getting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd59c44de9849498288d040369088f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9968), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Done\n",
      "Progress: 0%\n",
      "Progress: 30%\n",
      "Progress: 65%\n",
      "Progress: 100%\n",
      "CPU times: user 12.9 s, sys: 3.04 s, total: 16 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inter_c2, inter_c1, inter_c0 = get_intersection(Xtrain, vectorizer, 200, False, False)\n",
    "t_inter_c2, t_inter_c1, t_inter_c0 = get_intersection(Xtest, vectorizer, 200, False, False)\n",
    "#inter_c2, inter_c1 = np.load('yac_data/inter/inter_syn_c2.npy'), np.load('yac_data/inter/inter_syn_c1.npy')\n",
    "#inter_c0 = np.load('yac_data/inter/inter_syn_c0.npy')\n",
    "#t_inter_c2, t_inter_c1 = np.load('yac_data/inter/t_inter_syn_c2.npy'), np.load('yac_data/inter/t_inter_syn_c1.npy')\n",
    "#t_inter_c0 = np.load('yac_data/inter/t_inter_syn_c0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "validation_contexts = set(np.random.choice(np.unique(Xtrain.context_id), 3500, replace=False))\n",
    "validation_mask = Xtrain.context_id.isin(validation_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation = Xtrain[validation_mask].reset_index(drop=True)\n",
    "train = Xtrain[~validation_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2_res = np.concatenate([c2, inter_c2], axis=1)\n",
    "c1_res = np.concatenate([c1, inter_c1], axis=1)\n",
    "c0_res = np.concatenate([c0, inter_c0], axis=1)\n",
    "r_res = np.concatenate([r], axis=1)\n",
    "#intr_cont = np.concatenate([c2_c1, c1_c0, c0_c2], axis=1)\n",
    "\n",
    "c2_res.shape\n",
    "\n",
    "#t_c2_res = np.concatenate([t_c2, t_inter_c2], axis=1)\n",
    "#t_c1_res = np.concatenate([t_c1, t_inter_c1], axis=1)\n",
    "#t_c0_res = np.concatenate([t_c0, t_inter_c0], axis=1)\n",
    "#t_r_res = np.concatenate([t_r], axis=1)\n",
    "\n",
    "test_top = top[validation_mask]\n",
    "train_top = top[~validation_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_c2_res = c2_res[~validation_mask]\n",
    "train_c1_res = c1_res[~validation_mask]\n",
    "train_c0_res = c0_res[~validation_mask]\n",
    "train_r_res = r_res[~validation_mask]\n",
    "train_meta_x = meta_x[~validation_mask]\n",
    "train_top_c2 = top_c2[~validation_mask]\n",
    "train_top_c1 = top_c1[~validation_mask]\n",
    "train_top_c0 = top_c0[~validation_mask]\n",
    "train_top_r = top_r[~validation_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_c2_res = c2_res[validation_mask]\n",
    "test_c1_res = c1_res[validation_mask]\n",
    "test_c0_res = c0_res[validation_mask]\n",
    "test_r_res = r_res[validation_mask]\n",
    "test_meta_x = meta_x[validation_mask]\n",
    "test_top_c2 = top_c2[validation_mask]\n",
    "test_top_c1 = top_c1[validation_mask]\n",
    "test_top_c0 = top_c0[validation_mask]\n",
    "test_top_r = top_r[validation_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_meta_x_s = meta_x_s[validation_mask]\n",
    "train_meta_x_s = meta_x_s[~validation_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPool1D, Permute, Add\n",
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_model_with_meta_cnn(dim_c2=100, dim_c1=100, dim_c0=100, dim_r=100, meta_dim=8):\n",
    "    input_c2 = Input(shape=(dim_c2,))\n",
    "    input_c1 = Input(shape=(dim_c1,))\n",
    "    input_c0 = Input(shape=(dim_c0,))\n",
    "    input_r = Input(shape=(dim_r,))\n",
    "    input_meta = Input(shape=(meta_dim,))\n",
    "\n",
    "    dl_c2 = Dense(512, input_dim=1, activation='relu')(input_c2)\n",
    "    dl_c2 = Dropout(0.5)(dl_c2)\n",
    "    dl_c2 = Dense(1024, activation='relu')(dl_c2)\n",
    "    dl_c2 = BatchNormalization()(dl_c2)\n",
    "    \n",
    "    dl_c1 = Dense(512,input_dim=1, activation='relu')(input_c1)\n",
    "    dl_c1 = Dropout(0.5)(dl_c1)\n",
    "    dl_c1 = Dense(1024, activation='relu')(dl_c1)\n",
    "    dl_c1 = BatchNormalization()(dl_c1)\n",
    "    \n",
    "    dl_c0 = Dense(512, input_dim=1, activation='relu')(input_c0)\n",
    "    dl_c0 = Dropout(0.5)(dl_c0)\n",
    "    dl_c0 = Dense(1024, activation='relu')(dl_c0)\n",
    "    dl_c0 = BatchNormalization()(dl_c0)\n",
    "    \n",
    "    dl_r = Dense(512,input_dim=1, activation='relu')(input_r)\n",
    "    dl_r = Dropout(0.5)(dl_r)\n",
    "    dl_r = Dense(1024, activation='relu')(dl_r)\n",
    "    dl_r = BatchNormalization()(dl_r)\n",
    "    \n",
    "    dl_meta = Dense(256,input_dim=1, activation='relu')(input_meta)\n",
    "    dl_meta = Dropout(0.5)(dl_meta)\n",
    "    dl_meta = Dense(512, activation='relu')(dl_meta)\n",
    "    dl_meta = BatchNormalization()(dl_meta)\n",
    "    \n",
    "    out1 = Concatenate()([dl_c2, dl_c1, dl_c0, dl_r, dl_meta])\n",
    "    \n",
    "    x1 = Dense(2048, activation='relu')(out1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    x1 = Dense(2048, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    input_top = Input(shape=(20, 200, ))\n",
    "    \n",
    "    cl_t0 = Conv1D(200, 1, activation='relu')(input_top)\n",
    "    cl_t0 = MaxPooling1D(9)(cl_t0)\n",
    "    cl_t0 = Flatten()(cl_t0)\n",
    "    cl_t0 = Dropout(0.5)(cl_t0)\n",
    "    \n",
    "    cl_t1 = Conv1D(200, 2, activation='relu')(input_top)\n",
    "    cl_t1 = MaxPooling1D(6)(cl_t1)\n",
    "    cl_t1 = Flatten()(cl_t1)\n",
    "    cl_t1 = Dropout(0.5)(cl_t1)\n",
    "    \n",
    "    cl_t2 = Conv1D(200, 3, activation='relu')(input_top)\n",
    "    cl_t2 = MaxPooling1D(6)(cl_t2)\n",
    "    cl_t2 = Flatten()(cl_t2)\n",
    "    cl_t2 = Dropout(0.5)(cl_t2)\n",
    "    \n",
    "    cl_t3 = Conv1D(200, 4, activation='relu')(input_top)\n",
    "    cl_t3 = MaxPooling1D(6)(cl_t3)\n",
    "    cl_t3 = Flatten()(cl_t3)\n",
    "    cl_t3 = Dropout(0.5)(cl_t3)\n",
    "    \n",
    "    cl_t4 = Conv1D(200, 5, activation='relu')(input_top)\n",
    "    cl_t4 = MaxPooling1D(6)(cl_t4)\n",
    "    cl_t4 = Flatten()(cl_t4)\n",
    "    cl_t4 = Dropout(0.5)(cl_t4)\n",
    "    \n",
    "    cl_t5 = Conv1D(200, 6, activation='relu')(input_top)\n",
    "    cl_t5 = MaxPooling1D(6)(cl_t5)\n",
    "    cl_t5 = Flatten()(cl_t5)\n",
    "    cl_t5 = Dropout(0.5)(cl_t5)\n",
    "    \n",
    "    cl_t6 = Conv1D(200, 7, activation='relu')(input_top)\n",
    "    cl_t6 = MaxPooling1D(6)(cl_t6)\n",
    "    cl_t6 = Flatten()(cl_t6)\n",
    "    cl_t6 = Dropout(0.5)(cl_t6)\n",
    "    \n",
    "    out2 = Concatenate()([cl_t0, cl_t1, cl_t2, cl_t3, cl_t4, cl_t5])\n",
    "\n",
    "    x2 = Dense(2048, activation='relu')(out2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x2 = Dense(2048, activation='relu')(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    #x2 = Dense(3, activation='softmax')(x2)\n",
    "    \n",
    "    out = Concatenate()([x1, x2])\n",
    "  \n",
    "    x = Dense(2048, activation='relu')(out)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input=[input_c2, input_c1, input_c0, input_r, input_meta, \n",
    "                         input_top], output=x)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 20, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 512)          205312      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 512)          205312      input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 512)          205312      input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 512)          102912      input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 256)          2304        input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 20, 200)      40200       input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 19, 200)      80200       input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 18, 200)      120200      input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 17, 200)      160200      input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 16, 200)      200200      input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 15, 200)      240200      input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 512)          0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 512)          0           dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 512)          0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 512)          0           dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 256)          0           dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 2, 200)       0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 3, 200)       0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 3, 200)       0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 3, 200)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 3, 200)       0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 3, 200)       0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 1024)         525312      dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 1024)         525312      dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 1024)         525312      dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 1024)         525312      dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 512)          131584      dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 400)          0           max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 600)          0           max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 600)          0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 600)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 600)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 600)          0           max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 1024)         4096        dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 1024)         4096        dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 1024)         4096        dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 1024)         4096        dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 512)          2048        dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 400)          0           flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 600)          0           flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 600)          0           flatten_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 600)          0           flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 600)          0           flatten_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 600)          0           flatten_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 4608)         0           batch_normalization_25[0][0]     \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "                                                                 batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3400)         0           dropout_50[0][0]                 \n",
      "                                                                 dropout_51[0][0]                 \n",
      "                                                                 dropout_52[0][0]                 \n",
      "                                                                 dropout_53[0][0]                 \n",
      "                                                                 dropout_54[0][0]                 \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 2048)         9439232     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 2048)         6965248     concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 2048)         0           dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 2048)         0           dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 2048)         4196352     dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 2048)         4196352     dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 2048)         8192        dense_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 2048)         8192        dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 4096)         0           batch_normalization_30[0][0]     \n",
      "                                                                 batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 2048)         8390656     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 2048)         0           dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 4096)         8392704     dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4096)         16384       dense_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 3)            12291       batch_normalization_32[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 45,439,219\n",
      "Trainable params: 45,413,619\n",
      "Non-trainable params: 25,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_top_model_with_meta_cnn_old(c2_res.shape[1],\n",
    "                         c2_res.shape[1],\n",
    "                         c2_res.shape[1],\n",
    "                         r_res.shape[1],\n",
    "                         8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_model_with_meta_cnn_old(dim_c2=100, dim_c1=100, dim_c0=100, dim_r=100, meta_dim=8):\n",
    "    input_c2 = Input(shape=(dim_c2,))\n",
    "    input_c1 = Input(shape=(dim_c1,))\n",
    "    input_c0 = Input(shape=(dim_c0,))\n",
    "    input_r = Input(shape=(dim_r,))\n",
    "    input_meta = Input(shape=(meta_dim,))\n",
    "\n",
    "    dl_c2 = Dense(512, input_dim=1, activation='relu')(input_c2)\n",
    "    dl_c2 = Dropout(0.5)(dl_c2)\n",
    "    dl_c2 = Dense(1024, activation='relu')(dl_c2)\n",
    "    dl_c2 = BatchNormalization()(dl_c2)\n",
    "    \n",
    "    dl_c1 = Dense(512,input_dim=1, activation='relu')(input_c1)\n",
    "    dl_c1 = Dropout(0.5)(dl_c1)\n",
    "    dl_c1 = Dense(1024, activation='relu')(dl_c1)\n",
    "    dl_c1 = BatchNormalization()(dl_c1)\n",
    "    \n",
    "    dl_c0 = Dense(512, input_dim=1, activation='relu')(input_c0)\n",
    "    dl_c0 = Dropout(0.5)(dl_c0)\n",
    "    dl_c0 = Dense(1024, activation='relu')(dl_c0)\n",
    "    dl_c0 = BatchNormalization()(dl_c0)\n",
    "    \n",
    "    dl_r = Dense(512,input_dim=1, activation='relu')(input_r)\n",
    "    dl_r = Dropout(0.5)(dl_r)\n",
    "    dl_r = Dense(1024, activation='relu')(dl_r)\n",
    "    dl_r = BatchNormalization()(dl_r)\n",
    "    \n",
    "    dl_meta = Dense(256,input_dim=1, activation='relu')(input_meta)\n",
    "    dl_meta = Dropout(0.5)(dl_meta)\n",
    "    dl_meta = Dense(512, activation='relu')(dl_meta)\n",
    "    dl_meta = BatchNormalization()(dl_meta)\n",
    "    \n",
    "    out1 = Concatenate()([dl_c2, dl_c1, dl_c0, dl_r, dl_meta])\n",
    "    \n",
    "    x1 = Dense(2048, activation='relu')(out1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    x1 = Dense(2048, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    input_top = Input(shape=(20, 200, ))\n",
    "    \n",
    "    cl_t0 = Conv1D(200, 1, activation='relu')(input_top)\n",
    "    cl_t0 = MaxPooling1D(10)(cl_t0)\n",
    "    cl_t0 = Flatten()(cl_t0)\n",
    "    cl_t0 = Dropout(0.5)(cl_t0)\n",
    "    \n",
    "    cl_t1 = Conv1D(200, 2, activation='relu')(input_top)\n",
    "    cl_t1 = MaxPooling1D(10)(cl_t1)\n",
    "    cl_t1 = Flatten()(cl_t1)\n",
    "    cl_t1 = Dropout(0.5)(cl_t1)\n",
    "    \n",
    "    cl_t2 = Conv1D(200, 3, activation='relu')(input_top)\n",
    "    cl_t2 = MaxPooling1D(10)(cl_t2)\n",
    "    cl_t2 = Flatten()(cl_t2)\n",
    "    cl_t2 = Dropout(0.5)(cl_t2)\n",
    "    \n",
    "    cl_t3 = Conv1D(200, 4, activation='relu')(input_top)\n",
    "    cl_t3 = MaxPooling1D(10)(cl_t3)\n",
    "    cl_t3 = Flatten()(cl_t3)\n",
    "    cl_t3 = Dropout(0.5)(cl_t3)\n",
    "    \n",
    "    cl_t4 = Conv1D(200, 5, activation='relu')(input_top)\n",
    "    cl_t4 = MaxPooling1D(10)(cl_t4)\n",
    "    cl_t4 = Flatten()(cl_t4)\n",
    "    cl_t4 = Dropout(0.5)(cl_t4)\n",
    "    \n",
    "    cl_t5 = Conv1D(200, 6, activation='relu')(input_top)\n",
    "    cl_t5 = MaxPooling1D(10)(cl_t5)\n",
    "    cl_t5 = Flatten()(cl_t5)\n",
    "    cl_t5 = Dropout(0.5)(cl_t5)\n",
    "    \n",
    "    cl_t6 = Conv1D(200, 7, activation='relu')(input_top)\n",
    "    cl_t6 = MaxPooling1D(10)(cl_t6)\n",
    "    cl_t6 = Flatten()(cl_t6)\n",
    "    cl_t6 = Dropout(0.5)(cl_t6)\n",
    "    \n",
    "    out2 = Concatenate()([cl_t0, cl_t1, cl_t2, cl_t3, cl_t4, cl_t5, cl_t6])\n",
    "\n",
    "    x2 = Dense(2048, activation='relu')(out2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x2 = Dense(2048, activation='relu')(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    #x2 = Dense(3, activation='softmax')(x2)\n",
    "    \n",
    "    out = Concatenate()([x1, x2])\n",
    "  \n",
    "    x = Dense(4096, activation='relu')(out)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input=[input_c2, input_c1, input_c0, input_r, input_meta, \n",
    "                         input_top], output=x)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'yac_weights/np-task1-yac-weights_model_eranewOLDBIG6.{epoch:02d}-{val_acc:.2f}.hdf5', \n",
    "    monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=1, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                                              min_delta=0, \n",
    "                                              patience=5, \n",
    "                                              verbose=1, \n",
    "                                              mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77591 samples, validate on 19942 samples\n",
      "Epoch 1/30\n",
      "77591/77591 [==============================] - 110s 1ms/step - loss: 0.5809 - acc: 0.6859 - val_loss: 0.8163 - val_acc: 0.6064\n",
      "Epoch 2/30\n",
      "77591/77591 [==============================] - 110s 1ms/step - loss: 0.5735 - acc: 0.6900 - val_loss: 0.8206 - val_acc: 0.6065\n",
      "Epoch 3/30\n",
      "77591/77591 [==============================] - 110s 1ms/step - loss: 0.5676 - acc: 0.6937 - val_loss: 0.8223 - val_acc: 0.6049\n",
      "Epoch 4/30\n",
      "77591/77591 [==============================] - 110s 1ms/step - loss: 0.5642 - acc: 0.6961 - val_loss: 0.8223 - val_acc: 0.6059\n",
      "Epoch 5/30\n",
      "77591/77591 [==============================] - 110s 1ms/step - loss: 0.5583 - acc: 0.6986 - val_loss: 0.8226 - val_acc: 0.6063\n",
      "Epoch 6/30\n",
      "77591/77591 [==============================] - 110s 1ms/step - loss: 0.5560 - acc: 0.7010 - val_loss: 0.8263 - val_acc: 0.6069\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03254b50b8>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model.fit([train_c2_res, train_c1_res, train_c0_res, train_r_res, train_meta_x,train_top],\n",
    "          label[~validation_mask],\n",
    "          validation_data = ([test_c2_res, test_c1_res, test_c0_res, test_r_res, test_meta_x, test_top], label[validation_mask]),\n",
    "          nb_epoch=30, sample_weight=Xtrain.confidence.values[~validation_mask],\n",
    "          batch_size=64, \n",
    "          callbacks=[earlyStopping, learning_rate_reduction, checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9968/9968 [==============================] - 4s 355us/step\n"
     ]
    }
   ],
   "source": [
    "p = model.predict([t_c2_res,t_c1_res,t_c0_res, t_r_res, t_meta_x, t_top], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('subs/sub_NEW44.csv', index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('yac_weights/np-task1-yac-weights_model_era.06-0.60.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('yac_weights/88077_val.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('yac_weights/np-task1-yac-weights_model_eranewOLDBIG6.03-0.60.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19942/19942 [==============================] - 6s 301us/step\n"
     ]
    }
   ],
   "source": [
    "p = model.predict([test_c2_res, test_c1_res, test_c0_res, test_r_res, test_meta_x, test_top], verbose=1)\n",
    "\n",
    "validation['scores'] = p.dot([-1, 0, 1])\n",
    "sub = validation.groupby('context_id').apply(\n",
    "    lambda x: x.sort_values('scores', ascending=False).reply_id\n",
    ").reset_index(level=0)\n",
    "\n",
    "evaluate(sub, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nDCG(group):\n",
    "    denom = np.log2(2 + np.arange(len(group)))\n",
    "    IDCG = np.sum(group.sort_values('label', ascending=False).label.values / denom)\n",
    "    if IDCG == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        DCG = np.sum(group.label.values / denom)\n",
    "        return DCG / IDCG\n",
    "\n",
    "\n",
    "def evaluate(sub, truth):\n",
    "    return sub.join(\n",
    "        truth.set_index(['context_id', 'reply_id']).label,\n",
    "        how='left', on=['context_id', 'reply_id']\n",
    "    ).groupby('context_id').apply(nDCG).mean() * 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('yac_weights/87982_val.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('prob2.npy', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_new = get_top_model_with_meta_cnn_vanila_gru(c2_res.shape[1],\n",
    "                         c2_res.shape[1],\n",
    "                         c2_res.shape[1],\n",
    "                         r_res.shape[1],\n",
    "                         8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'yac_weights/np-task1-yac-weights_model_eranewNEWNEW3.{epoch:02d}-{val_acc:.2f}.hdf5', \n",
    "    monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=1, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.3, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                                              min_delta=0, \n",
    "                                              patience=5, \n",
    "                                              verbose=1, \n",
    "                                              mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_183 (InputLayer)          (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_184 (InputLayer)          (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_185 (InputLayer)          (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_186 (InputLayer)          (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_187 (InputLayer)          (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_188 (InputLayer)          (None, 20, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_325 (Dense)               (None, 256)          102656      input_183[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_327 (Dense)               (None, 256)          102656      input_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_329 (Dense)               (None, 256)          102656      input_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_331 (Dense)               (None, 256)          51456       input_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_333 (Dense)               (None, 64)           576         input_187[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 19, 100)      40100       input_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 18, 100)      60100       input_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 256)          1024        dense_325[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 256)          1024        dense_327[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 256)          1024        dense_329[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 256)          1024        dense_331[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 64)           256         dense_333[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling1D) (None, 6, 100)       0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling1D) (None, 6, 100)       0           conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_326 (Dense)               (None, 512)          131584      batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_328 (Dense)               (None, 512)          131584      batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_330 (Dense)               (None, 512)          131584      batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_332 (Dense)               (None, 512)          131584      batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_334 (Dense)               (None, 128)          8320        batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_59 (Flatten)            (None, 600)          0           max_pooling1d_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_60 (Flatten)            (None, 600)          0           max_pooling1d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 2176)         0           dense_326[0][0]                  \n",
      "                                                                 dense_328[0][0]                  \n",
      "                                                                 dense_330[0][0]                  \n",
      "                                                                 dense_332[0][0]                  \n",
      "                                                                 dense_334[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 1200)         0           flatten_59[0][0]                 \n",
      "                                                                 flatten_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_335 (Dense)               (None, 1024)         2229248     concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_336 (Dense)               (None, 512)          614912      concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_192 (Dropout)           (None, 1024)         0           dense_335[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_193 (Dropout)           (None, 512)          0           dense_336[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 1536)         0           dropout_192[0][0]                \n",
      "                                                                 dropout_193[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_337 (Dense)               (None, 2048)         3147776     concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_194 (Dropout)           (None, 2048)         0           dense_337[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_338 (Dense)               (None, 2048)         4196352     dropout_194[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 2048)         8192        dense_338[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_339 (Dense)               (None, 3)            6147        batch_normalization_148[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 11,201,835\n",
      "Trainable params: 11,195,563\n",
      "Non-trainable params: 6,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_new.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77591 samples, validate on 19942 samples\n",
      "Epoch 1/30\n",
      "77591/77591 [==============================] - 39s 498us/step - loss: 0.7380 - acc: 0.5691 - val_loss: 0.8829 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "77591/77591 [==============================] - 38s 486us/step - loss: 0.7144 - acc: 0.5918 - val_loss: 0.8633 - val_acc: 0.5609\n",
      "Epoch 3/30\n",
      "77591/77591 [==============================] - 38s 485us/step - loss: 0.7047 - acc: 0.6064 - val_loss: 0.8447 - val_acc: 0.5815\n",
      "Epoch 4/30\n",
      "77591/77591 [==============================] - 38s 485us/step - loss: 0.6838 - acc: 0.6228 - val_loss: 0.8163 - val_acc: 0.6032\n",
      "Epoch 5/30\n",
      "77591/77591 [==============================] - 38s 484us/step - loss: 0.6510 - acc: 0.6403 - val_loss: 0.8504 - val_acc: 0.5986\n",
      "Epoch 6/30\n",
      "77591/77591 [==============================] - 38s 487us/step - loss: 0.6053 - acc: 0.6664 - val_loss: 1.2073 - val_acc: 0.4160\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 7/30\n",
      "77591/77591 [==============================] - 38s 484us/step - loss: 0.5076 - acc: 0.7274 - val_loss: 0.8908 - val_acc: 0.6003\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 8/30\n",
      "77591/77591 [==============================] - 37s 482us/step - loss: 0.4372 - acc: 0.7709 - val_loss: 0.9606 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "Epoch 9/30\n",
      "77591/77591 [==============================] - 38s 484us/step - loss: 0.4066 - acc: 0.7908 - val_loss: 1.0061 - val_acc: 0.5922\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f260923dfd0>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "np.random.RandomState(42)\n",
    "model_new.fit([train_c2_res, train_c1_res, train_c0_res, train_r_res, train_meta_x, train_top, \n",
    "           train_top_c0, train_top_r],\n",
    "          label[~validation_mask],\n",
    "          validation_data = ([test_c2_res, test_c1_res, test_c0_res, test_r_res, test_meta_x,\n",
    "                              test_top, test_top_c0, test_top_r], label[validation_mask]),\n",
    "          nb_epoch=30, sample_weight=Xtrain.confidence.values[~validation_mask],\n",
    "          batch_size=78, \n",
    "          callbacks=[earlyStopping, learning_rate_reduction, checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_new.load_weights('yac_weights/np-task1-yac-weights_model_eranewNEWNEW3.05-0.60.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19942/19942 [==============================] - 5s 227us/step\n"
     ]
    }
   ],
   "source": [
    "p = model_new.predict([test_c2_res, test_c1_res, test_c0_res, test_r_res, test_meta_x,\n",
    "                   test_top,  test_top_c0, test_top_r], verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87322.86650461525"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation['scores'] = p.dot([-1, 0, 1])\n",
    "sub = validation.groupby('context_id').apply(\n",
    "    lambda x: x.sort_values('scores', ascending=False).reply_id\n",
    ").reset_index(level=0)\n",
    "\n",
    "evaluate(sub, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('yac_weights/87607_val.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('yac_weights/np-task1-yac-weights_model_eranew.06-0.60.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 6 array(s), but instead got the following list of 9 arrays: [array([[ 0.05075625,  0.02434491,  0.02742608, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.05075625,  0.02434491,  0.02742608, ...,  0.        ,\n         0.        ,  0.        ],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-efb7608ddb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m p = model.predict([test_c2_res,test_c1_res,test_c0_res, test_r_res, test_meta_x,\n\u001b[0;32m----> 2\u001b[0;31m                     test_top_c2, test_top_c1, test_top_c0, test_top_r], verbose=1)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sub = validation.groupby('context_id').apply(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 6 array(s), but instead got the following list of 9 arrays: [array([[ 0.05075625,  0.02434491,  0.02742608, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.05075625,  0.02434491,  0.02742608, ...,  0.        ,\n         0.        ,  0.        ],..."
     ]
    }
   ],
   "source": [
    "p = model.predict([test_c2_res,test_c1_res,test_c0_res, test_r_res, test_meta_x,\n",
    "                    test_top_c2, test_top_c1, test_top_c0, test_top_r], verbose=1)\n",
    "\n",
    "validation['scores'] = p.dot([-1, 0, 1])\n",
    "sub = validation.groupby('context_id').apply(\n",
    "    lambda x: x.sort_values('scores', ascending=False).reply_id\n",
    ").reset_index(level=0)\n",
    "\n",
    "evaluate(sub, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19942/19942 [==============================] - 10s 503us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([test_c2_res,test_c1_res,test_c0_res, test_r_res, test_meta_x,\n",
    "                    test_top_c2, test_top_c1, test_top_c0, test_top_r], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('subs/sub_new_OLD_META2.csv', index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0bb3d56aa6a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Gettign embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnb_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "for index, filtersize in enumerate(filtersize_list):\n",
    "    nb_filter = number_of_filters_per_filtersize[index]\n",
    "    pool_length = pool_length_list[index]\n",
    "    conv = Conv1D(nb_filter=nb_filter, filter_length=filtersize, activation='relu')(input_context)\n",
    "    pool = MaxPooling1D(pool_length=pool_length)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    flatten = Dropout(0.5)(flatten)\n",
    "    conv_list_context.append(flatten)\n",
    "    \n",
    "for index, filtersize in enumerate(filtersize_list):\n",
    "    nb_filter = number_of_filters_per_filtersize[index]\n",
    "    pool_length = pool_length_list[index]\n",
    "    conv = Conv1D(nb_filter=nb_filter, filter_length=filtersize, activation='relu')(input_reply)\n",
    "    pool = MaxPooling1D(pool_length=pool_length)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    flatten = Dropout(0.5)(flatten)\n",
    "    conv_list_reply.append(flatten)\n",
    "\n",
    "out_context = Merge(mode='concat')(conv_list_context)\n",
    "out_reply = Merge(mode='concat')(conv_list_reply)\n",
    "\n",
    "out = Concatenate()([out_context, out_reply])\n",
    "\n",
    "x = Dropout(0.5)(out)\n",
    "x = Dense(100)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(input=[input_1, input_2], output=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "model.fit([train_c, train_r], label,\n",
    "          validation_split=0.15,\n",
    "          nb_epoch=5,\n",
    "          batch_size=64, \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Convolution1D,  Convolution2D, MaxPooling1D, Embedding, Reshape, LSTM, BatchNormalization\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Merge, Activation, Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 11, 128)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 11, 128)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 9, 30)        11550       input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 4, 30)        0           conv1d_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 270)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 120)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 390)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           3910        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,471\n",
      "Trainable params: 15,471\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "  14/1000 [..............................] - ETA: 1:50:36 - loss: 1.9401 - acc: 0.5134"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f98c9875c3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m                    \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_speakers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAMPLE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                    validation_steps = 100)\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Make sure to rethrow the first exception in the queue, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder = '../tf_dataset_features_npy/'\n",
    "\n",
    "speakers_list = os.listdir(folder)\n",
    "speakers_set = set(speakers_list)\n",
    "test_speakers = np.random.choice(speakers_list, 300, replace=False)\n",
    "train_speakers = np.array(list(speakers_set - set(test_speakers)))\n",
    "\n",
    "def create_sample_from_audio(file, sample_length=11):\n",
    "    words_list = os.listdir(file)\n",
    "    word = np.random.choice(words_list, 1)[0]\n",
    "    samples_list = os.listdir(os.path.join(file, word))\n",
    "    sample = np.random.choice(samples_list, 1)[0]\n",
    "    features = np.load(os.path.join(file, word, sample))\n",
    "    len = features.shape[1]\n",
    "    i = np.random.randint(sample_length, len)\n",
    "    sample = features[:, i - sample_length : i]\n",
    "    #print(sample.shape)\n",
    "    return sample#.swapaxes(0, 1)\n",
    "\n",
    "def batch_generator(folder, speakers_list, batch_size=32, sample_length=11):\n",
    "    people_files = os.listdir(folder)\n",
    "    while True:\n",
    "        x_batch1, x_batch2, y_batch = [], [],[]\n",
    "        for i in range(batch_size):\n",
    "            if np.random.uniform(0,1) < 0.5: #from one man\n",
    "                speaker = np.random.choice(speakers_list, 1)[0]\n",
    "                sample_part1 = create_sample_from_audio(os.path.join(folder, speaker), sample_length)\n",
    "                sample_part2 = create_sample_from_audio(os.path.join(folder, speaker), sample_length)\n",
    "                y = 1\n",
    "            else: #from two different people\n",
    "                speaker1, speaker2 = np.random.choice(speakers_list, 2, replace=False)  \n",
    "                sample_part1 = create_sample_from_audio(os.path.join(folder, speaker1), sample_length)\n",
    "                sample_part2 = create_sample_from_audio(os.path.join(folder, speaker2), sample_length)\n",
    "                y = 0\n",
    "                \n",
    "            x_batch1.append(sample_part1.reshape(11, 128))\n",
    "            x_batch2.append(sample_part2.reshape(11, 128))\n",
    "            y_batch.append(y)\n",
    "            \n",
    "        x_batch1 = np.array(x_batch1)\n",
    "        x_batch2 = np.array(x_batch2)\n",
    "        y_batch = np.array(y_batch)\n",
    "\n",
    "        yield [x_batch1, x_batch2], y_batch\n",
    "\n",
    "def generate_model(sample_length=11):\n",
    "    inputs1 = Input(shape=(11, 128, ))\n",
    "    inputs2 = Input(shape=(11, 128, ))\n",
    "    \n",
    "    conv_layer = Conv1D(nb_filter=30, filter_length=3,  activation = 'relu')\n",
    "    conv1 = conv_layer(inputs1)\n",
    "    conv2 = conv_layer(inputs2)\n",
    "    \n",
    "    pool1 = MaxPooling1D()(conv1)\n",
    "    pool2 = MaxPooling1D()(conv2)\n",
    "    pool1 = Flatten()(conv1)\n",
    "    pool2 = Flatten()(pool2)\n",
    "    merged = Concatenate()([pool1, pool2])\n",
    "    \n",
    "    classify = Dense(10, activation = 'relu')(merged)\n",
    "    classify = Dense(1, activation = 'sigmoid')(classify)\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=classify)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "SAMPLE_LENGTH = 11\n",
    "model = generate_model(SAMPLE_LENGTH)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "model.fit_generator(batch_generator(folder, train_speakers, 128, SAMPLE_LENGTH),\n",
    "                   steps_per_epoch =1000,\n",
    "                   nb_epoch = 100,\n",
    "                   validation_data = batch_generator(folder, test_speakers, 2, SAMPLE_LENGTH),\n",
    "                   validation_steps = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=(11, 128,))\n",
    "conv1 = Conv1D(30, 3,  activation = 'relu')(inputs1)\n",
    "conv1 = Flatten()(conv1)\n",
    "classify = Dense(1, activation = 'sigmoid')(conv1)\n",
    "model = Model(inputs=[inputs1], outputs=classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.zeros((1, 11, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.zeros((1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\r",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.6931 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a7afa0780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
